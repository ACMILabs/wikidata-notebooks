{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 4007/4901 [00:25<00:14, 60.34it/s]  "
     ]
    }
   ],
   "source": [
    "\n",
    "# detect matches based on linked creator -> exact work title -> exact creator name.\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import unidecode\n",
    "\n",
    "def value_extract(row, column):\n",
    "\n",
    "    ''' Extract dictionary values. '''\n",
    "    \n",
    "    return pydash.get(row[column], 'value')\n",
    "\n",
    "def sparql_query(query, service):\n",
    "\n",
    "    ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "    response = requests.get(service, params={'format': 'json', 'query': query}, timeout=120)\n",
    "    results = pydash.get(response.json(), 'results.bindings')\n",
    "    data_frame = pandas.DataFrame.from_dict(results)\n",
    "    for column in data_frame.columns:\n",
    "        data_frame[column] = data_frame.apply(value_extract, column=column, axis=1)\n",
    "    \n",
    "    return data_frame\n",
    "\n",
    "def wikidata_connections(wikidata_subject_id):\n",
    "\n",
    "    wikidata_query = ''' \n",
    "        select ?subject ?subjectLabel ?work ?workLabel ?colleague ?colleagueLabel where { \n",
    "            values ?subject {wd:'''+wikidata_subject_id+'''}\n",
    "            ?work ?prop ?subject .\n",
    "            ?work ?prop ?colleague .\n",
    "            service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "            } '''\n",
    "\n",
    "    wikidata_df = sparql_query(wikidata_query, 'https://query.wikidata.org/sparql')\n",
    "    if len(wikidata_df):\n",
    "        wikidata_df = wikidata_df.loc[wikidata_df.subject != wikidata_df.colleague]\n",
    "        wikidata_df = wikidata_df[['work', 'workLabel', 'colleague', 'colleagueLabel']]\n",
    "        wikidata_df = wikidata_df.rename(columns={'work': 'wikidata_work', 'colleague': 'wikidata_colleague'})\n",
    "        wikidata_df['wikidata_work'] = wikidata_df['wikidata_work'].str.split('/').str[-1]\n",
    "        wikidata_df['wikidata_colleague'] = wikidata_df['wikidata_colleague'].str.split('/').str[-1]\n",
    "\n",
    "        return wikidata_df\n",
    "    else:\n",
    "        return pandas.DataFrame() \n",
    "\n",
    "def acmi_connections(acmi_subject_id):\n",
    "\n",
    "    acmi_works = pandas.read_csv(pathlib.Path.cwd().parents[0] / 'acmi-api' / 'app' / 'tsv' / 'works.tsv', delimiter='\\t', low_memory=False)\n",
    "    acmi_works = pandas.concat([\n",
    "        acmi_works[['id', 'title', 'creators_primary']].rename(columns={'creators_primary':'creators'}),\n",
    "        acmi_works[['id', 'title', 'creators_other']].rename(columns={'creators_other':'creators'})])\n",
    "\n",
    "    acmi_works['creators'] = acmi_works['creators'].str.split(',')\n",
    "    acmi_works = acmi_works.explode('creators')\n",
    "    acmi_works['creators'] = acmi_works['creators'].str.strip()\n",
    "    acmi_works = acmi_works.dropna(subset='creators')\n",
    "\n",
    "    acmi_creators = pandas.read_csv(pathlib.Path.cwd().parents[0] / 'acmi-api' / 'app' / 'tsv' / 'creators.tsv', delimiter='\\t', low_memory=False)\n",
    "    acmi_creators = acmi_creators[['id', 'name']].drop_duplicates().astype(str)\n",
    "\n",
    "    acmi_df = acmi_creators.loc[acmi_creators.id.isin(['73769'])].rename(columns={'id':'subject', 'name':'subjectLabel'})\n",
    "    acmi_df = pandas.merge(acmi_df, acmi_works.rename(columns={'creators':'subject', 'id':'work', 'title':'workLabel'}), on='subject', how='left')\n",
    "    acmi_df = pandas.merge(acmi_df, acmi_works.rename(columns={'creators':'colleague', 'id':'work', 'title':'workLabel'}), on=['work', 'workLabel'], how='left')\n",
    "\n",
    "    acmi_df = pandas.merge(acmi_df, acmi_creators.rename(columns={'id':'colleague', 'name':'colleagueLabel'}), on=['colleague'], how='left')\n",
    "\n",
    "    acmi_df = acmi_df.loc[acmi_df.subject != acmi_df.colleague]\n",
    "    acmi_df = acmi_df[['work', 'workLabel', 'colleague', 'colleagueLabel']]\n",
    "    acmi_df = acmi_df.rename(columns={'work': 'acmi_work', 'colleague': 'acmi_colleague'})\n",
    "\n",
    "    return acmi_df\n",
    "\n",
    "def detect_connections(wikidata_creator_id, acmi_creator_id, extant):\n",
    "\n",
    "    wikidata_data = wikidata_connections(wikidata_creator_id)\n",
    "    if len(wikidata_data):\n",
    "        acmi_data = acmi_connections(acmi_creator_id)\n",
    "\n",
    "        mashup = pandas.merge(acmi_data, wikidata_data, on=['workLabel', 'colleagueLabel'], how='inner')\n",
    "        mashup['acmi_work'] = 'works/'+mashup['acmi_work'].astype(str)\n",
    "        mashup['acmi_colleague'] = 'creators/'+mashup['acmi_colleague'].astype(str)\n",
    "\n",
    "        links = pandas.concat([\n",
    "            mashup[['wikidata_colleague', 'acmi_colleague']].rename(\n",
    "                columns={'wikidata_colleague': 'wikidata_id', 'acmi_colleague': 'acmi_id'}),\n",
    "            mashup[['wikidata_work', 'acmi_work']].rename(\n",
    "                columns={'wikidata_work': 'wikidata_id', 'acmi_work': 'acmi_id'}),\n",
    "            ]).drop_duplicates()\n",
    "\n",
    "        links = pandas.merge(links, extant, indicator=True, how='left').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "        return links\n",
    "    else:\n",
    "        return pandas.DataFrame()\n",
    "\n",
    "def extant_connections():\n",
    "\n",
    "    extant_query = ''' select distinct ?wikidata_id ?acmi_id where { ?wikidata_id wdt:P7003 ?acmi_id } '''\n",
    "    extant_links = sparql_query(extant_query, 'https://query.wikidata.org/sparql')\n",
    "    extant_links['wikidata_id'] = extant_links['wikidata_id'].str.split('/').str[-1]\n",
    "\n",
    "    return extant_links\n",
    "\n",
    "extant_df = extant_connections()\n",
    "extant_creators = extant_df.copy()\n",
    "extant_creators = extant_creators.loc[extant_creators.acmi_id.str.contains('creators', na=False)]\n",
    "\n",
    "for x in tqdm.tqdm(extant_creators.to_dict('records')):\n",
    "    link_hash = hashlib.md5(x['wikidata_id'].encode()).hexdigest()\n",
    "    summary_path = pathlib.Path.cwd().parents[0] / 'data' / 'creator_iterator' / f'{link_hash}.csv'\n",
    "    summary_path.parents[0].mkdir(exist_ok=True, parents=True)\n",
    "    if not summary_path.exists():\n",
    "        res = detect_connections(x['wikidata_id'], x['acmi_id'], extant_df)\n",
    "        res.to_csv(summary_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
