{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338757\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creator</th>\n",
       "      <th>creatorLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q261</td>\n",
       "      <td>linkin park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q272</td>\n",
       "      <td>paul morand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1225</td>\n",
       "      <td>bruce springsteen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q11319</td>\n",
       "      <td>david decoteau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q181</td>\n",
       "      <td>jimmy wales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 creator       creatorLabel\n",
       "0    http://www.wikidata.org/entity/Q261        linkin park\n",
       "1    http://www.wikidata.org/entity/Q272        paul morand\n",
       "2   http://www.wikidata.org/entity/Q1225  bruce springsteen\n",
       "3  http://www.wikidata.org/entity/Q11319     david decoteau\n",
       "4    http://www.wikidata.org/entity/Q181        jimmy wales"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of wikidata creators\n",
    "# then using acmi creator list find instance in which there are no reasonable matches\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "import hashlib\n",
    "import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import unidecode\n",
    "\n",
    "def value_extract(row, column):\n",
    "\n",
    "    ''' Extract dictionary values. '''\n",
    "    \n",
    "    return pydash.get(row[column], 'value')\n",
    "\n",
    "def sparql_query(query, service):\n",
    "\n",
    "    ''' Send sparql request, and formulate results into a dataframe. '''\n",
    "\n",
    "    response = requests.get(service, params={'format': 'json', 'query': query}, timeout=120)\n",
    "    results = pydash.get(response.json(), 'results.bindings')\n",
    "    df = pandas.DataFrame.from_dict(results)\n",
    "    for column in df.columns:\n",
    "        df[column] = df.apply(value_extract, column=column, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def normalise_string(input_text):\n",
    "\n",
    "    ''' Normalise text for matching purposes. '''\n",
    "\n",
    "    return unidecode.unidecode(str(input_text).lower()).strip()\n",
    "\n",
    "wikidata_creator_data = pathlib.Path.cwd().parents[0] / 'data' / 'creator_match' / 'wikidata_creator.parquet'\n",
    "if not wikidata_creator_data.exists():\n",
    "    query = '''\n",
    "    select distinct ?creator \n",
    "        where {\n",
    "            {?work wdt:P57 ?creator . } union\n",
    "            {?work wdt:P58 ?creator . } union\n",
    "            {?work wdt:P161 ?creator . } union\n",
    "            {?work wdt:P272 ?creator . } union\n",
    "            {?work wdt:P344 ?creator . } union\n",
    "            {?work wdt:P1040 ?creator . } union\n",
    "            {?work wdt:P2515 ?creator . } union\n",
    "            {?work wdt:P2554 ?creator . } \n",
    "        } '''\n",
    "\n",
    "    wikidata_creators = sparql_query(query, 'https://query.wikidata.org/sparql').drop_duplicates()\n",
    "    wikidata_creators['creator'] = wikidata_creators['creator'].str.split('/').str[-1]\n",
    "\n",
    "    wikidata_dataframe = pandas.DataFrame()\n",
    "    for chunk in tqdm.tqdm(numpy.array_split(wikidata_creators.creator.unique(), 2000)):\n",
    "        time.sleep(4)\n",
    "        query = '''\n",
    "            select distinct ?creator ?creatorLabel\n",
    "            where {\n",
    "                values ?creator {'''+' '.join([f'wd:{x}' for x in chunk])+'''}\n",
    "                service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "            } '''\n",
    "        wikidata_dataframe = pandas.concat([wikidata_dataframe, sparql_query(query, 'https://query.wikidata.org/sparql')])\n",
    "    wikidata_dataframe.to_parquet(wikidata_creator_data)\n",
    "else:\n",
    "    wikidata_dataframe = pandas.read_parquet(wikidata_creator_data)\n",
    "\n",
    "wikidata_dataframe['creatorLabel'] = wikidata_dataframe['creatorLabel'].str.lower()\n",
    "\n",
    "print(len(wikidata_dataframe))\n",
    "wikidata_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52400147\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nconst</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nm1588970</td>\n",
       "      <td>Carmencita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nm0005690</td>\n",
       "      <td>Carmencita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nm0374658</td>\n",
       "      <td>Carmencita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nm0721526</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nm1335271</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nconst                   title\n",
       "0  nm1588970              Carmencita\n",
       "1  nm0005690              Carmencita\n",
       "2  nm0374658              Carmencita\n",
       "3  nm0721526  Le clown et ses chiens\n",
       "4  nm1335271  Le clown et ses chiens"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imdb dataframe\n",
    "\n",
    "# what do you need to know, nameconst, connected to names of films, try jacki weaver\n",
    "\n",
    "\n",
    "imdb_data_path = pathlib.Path.cwd().parents[0] / 'data' / 'creator_match' / 'imdb_data.parquet'\n",
    "if not imdb_data_path.exists():\n",
    "    imdb_data = pandas.read_csv(pathlib.Path.home() / 'imdb' / 'title.principals.tsv', delimiter='\\t', low_memory=False)\n",
    "    imdb_data = pandas.merge(imdb_data, pandas.read_csv(pathlib.Path.home() / 'imdb' / 'title.basics.tsv', delimiter='\\t', low_memory=False), on='tconst', how='left')\n",
    "    imdb_data = pandas.concat([\n",
    "        imdb_data[['nconst', 'primaryTitle']].rename(columns={'primaryTitle':'title'}),\n",
    "        imdb_data[['nconst', 'originalTitle']].rename(columns={'originalTitle':'title'})]).drop_duplicates()\n",
    "    imdb_data.to_parquet(imdb_data_path, index=False)\n",
    "else:\n",
    "    imdb_data = pandas.read_parquet(imdb_data_path)\n",
    "    \n",
    "print(len(imdb_data))\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144784\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>creator_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>works/119934</td>\n",
       "      <td>The Dame Was Loaded German advertisement</td>\n",
       "      <td>creators/41813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>works/115143</td>\n",
       "      <td>World Is Ours</td>\n",
       "      <td>creators/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>works/90799</td>\n",
       "      <td>Wing Chun</td>\n",
       "      <td>creators/32508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>works/90495</td>\n",
       "      <td>The Flying doctor</td>\n",
       "      <td>creators/11967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>works/90495</td>\n",
       "      <td>The Flying doctor</td>\n",
       "      <td>creators/12786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                     title      creator_id\n",
       "0  works/119934  The Dame Was Loaded German advertisement  creators/41813\n",
       "1  works/115143                             World Is Ours       creators/\n",
       "2   works/90799                                 Wing Chun  creators/32508\n",
       "3   works/90495                         The Flying doctor  creators/11967\n",
       "3   works/90495                         The Flying doctor  creators/12786"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acmi_works = pandas.read_csv(pathlib.Path.cwd().parents[0] / 'acmi-api' / 'app' / 'tsv' / 'works.tsv', delimiter='\\t', low_memory=False)\n",
    "acmi_works = pandas.concat([\n",
    "    acmi_works[['id', 'title', 'creators_primary']].rename(columns={'creators_primary':'creator_id'}),\n",
    "    acmi_works[['id', 'title', 'creators_other']].rename(columns={'creators_other':'creator_id'})\n",
    "])\n",
    "\n",
    "acmi_works['creator_id'] = acmi_works['creator_id'].str.split(',')\n",
    "acmi_works = acmi_works.explode('creator_id')\n",
    "acmi_works['creator_id'] = acmi_works['creator_id'].str.strip()\n",
    "acmi_works = acmi_works.drop_duplicates().fillna('')\n",
    "acmi_works['id'] = 'works/'+acmi_works['id'].astype(str)\n",
    "acmi_works['creator_id'] = 'creators/'+acmi_works['creator_id'].astype(str)\n",
    "\n",
    "print(len(acmi_works))\n",
    "acmi_works.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 563/17616 [55:34<28:03:26,  5.92s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m imdb_page_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(imdb_page_result) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m         imdb_data_candidate \u001b[39m=\u001b[39m imdb_data\u001b[39m.\u001b[39mloc[imdb_data\u001b[39m.\u001b[39;49mnconst\u001b[39m.\u001b[39;49misin([imdb_page_result[\u001b[39m0\u001b[39;49m]])]\n\u001b[1;32m    108\u001b[0m         \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m acmi_filmography:\n\u001b[1;32m    109\u001b[0m             \u001b[39mif\u001b[39;00m g \u001b[39min\u001b[39;00m imdb_data_candidate\u001b[39m.\u001b[39mtitle\u001b[39m.\u001b[39munique():\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:5407\u001b[0m, in \u001b[0;36mSeries.isin\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   5334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misin\u001b[39m(\u001b[39mself\u001b[39m, values) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m   5335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5336\u001b[0m \u001b[39m    Whether elements in Series are contained in `values`.\u001b[39;00m\n\u001b[1;32m   5337\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5405\u001b[0m \u001b[39m    dtype: bool\u001b[39;00m\n\u001b[1;32m   5406\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5407\u001b[0m     result \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49misin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, values)\n\u001b[1;32m   5408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(result, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39m__finalize__(\n\u001b[1;32m   5409\u001b[0m         \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39misin\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5410\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/algorithms.py:530\u001b[0m, in \u001b[0;36misin\u001b[0;34m(comps, values)\u001b[0m\n\u001b[1;32m    527\u001b[0m     comps_array \u001b[39m=\u001b[39m comps_array\u001b[39m.\u001b[39mastype(common, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    528\u001b[0m     f \u001b[39m=\u001b[39m htable\u001b[39m.\u001b[39mismember\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m f(comps_array, values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "query = '''\n",
    "    select ?acmi_id ?wikidata_id\n",
    "    where {\n",
    "        ?wikidata_id wdt:P7003 ?acmi_id .\n",
    "        filter(regex(str(?acmi_id), \"creators\")) .\n",
    "        } '''\n",
    "\n",
    "extant_links = sparql_query(query, 'https://query.wikidata.org/sparql').drop_duplicates()\n",
    "acmi_creators = pandas.read_csv(pathlib.Path.cwd().parents[0] / 'acmi-api' / 'app' / 'tsv' / 'creators.tsv', delimiter='\\t', low_memory=False)\n",
    "acmi_creators = acmi_creators[['id', 'name']].rename(columns={'id':'acmi_id', 'name':'acmi_label'})\n",
    "acmi_creators['acmi_id'] = 'creators/'+acmi_creators['acmi_id'].astype(str)\n",
    "acmi_creators = acmi_creators.loc[~acmi_creators.acmi_id.isin(list(extant_links.acmi_id))]\n",
    "\n",
    "# ideal here is to add increasing checks\n",
    "\n",
    "def wikidata_titles(wikidata):\n",
    "\n",
    "    query = '''\n",
    "    select distinct ?workLabel \n",
    "    where {\n",
    "        values ?creator {wd:'''+wikidata+'''}\n",
    "        {?work wdt:P57 ?creator . } union\n",
    "        {?work wdt:P58 ?creator . } union\n",
    "        {?work wdt:P161 ?creator . } union\n",
    "        {?work wdt:P272 ?creator . } union\n",
    "        {?work wdt:P344 ?creator . } union\n",
    "        {?work wdt:P1040 ?creator . } union\n",
    "        {?work wdt:P2515 ?creator . } union\n",
    "        {?work wdt:P2554 ?creator . } \n",
    "        service wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "    } '''\n",
    "\n",
    "    titles = sparql_query(query, 'https://query.wikidata.org/sparql')\n",
    "    if len(titles):\n",
    "        return titles.workLabel.unique()\n",
    "\n",
    "def wikipedia_page(wikidata):\n",
    "\n",
    "    query = '''\n",
    "    select distinct ?article\n",
    "    where { \n",
    "        values ?creator {wd:'''+wikidata+'''} .\n",
    "        ?article schema:about ?creator .\n",
    "        ?article schema:isPartOf <https://en.wikipedia.org/>  .\n",
    "    } '''\n",
    "    \n",
    "    titles = sparql_query(query, 'https://query.wikidata.org/sparql')\n",
    "    if len(titles):\n",
    "        return titles.article.unique()\n",
    "\n",
    "def imdb_page(wikidata):\n",
    "    \n",
    "    query = '''\n",
    "    select distinct ?imdb\n",
    "    where { \n",
    "        values ?creator {wd:'''+wikidata+'''} .\n",
    "        ?creator wdt:P345 ?imdb .\n",
    "    } '''\n",
    "    \n",
    "    titles = sparql_query(query, 'https://query.wikidata.org/sparql')\n",
    "    if len(titles):\n",
    "        return titles.imdb.unique()\n",
    "\n",
    "\n",
    "creator_result = pandas.DataFrame(columns=['acmi', 'wikidata'])\n",
    "\n",
    "for creator in tqdm.tqdm(acmi_creators.to_dict('records')):\n",
    "    \n",
    "    hash_id = hashlib.md5(creator['acmi_id'].encode()).hexdigest()\n",
    "    hash_path = pathlib.Path.cwd().parents[0] / 'data' / 'creator_match' / f'{hash_id}.txt'\n",
    "\n",
    "    if not hash_path.exists():\n",
    "\n",
    "        c = process.extract(creator['acmi_label'], wikidata_dataframe.creatorLabel.unique(), scorer=fuzz.WRatio, limit=20)\n",
    "        candidates = [x[0] for x in c if x[1] > 75] \n",
    "\n",
    "        match = ''\n",
    "\n",
    "        if len(candidates):\n",
    "\n",
    "            acmi_filmography = acmi_works.copy()\n",
    "            acmi_filmography = acmi_filmography.loc[acmi_filmography.creator_id.isin([creator['acmi_id']])]\n",
    "            acmi_filmography = [x['title'] for x in acmi_filmography.to_dict('records')]\n",
    "\n",
    "            candidate_dataframe = wikidata_dataframe.copy()\n",
    "            candidate_dataframe = candidate_dataframe.loc[candidate_dataframe.creatorLabel.isin(candidates)]\n",
    "\n",
    "            for wikidata_candidate in candidate_dataframe.creator.unique():\n",
    "\n",
    "                wikidata_id = wikidata_candidate.split('/')[-1]\n",
    "                wikidata_titles_array = wikidata_titles(wikidata_id)\n",
    "                film_results = [process.extractOne(f[1], wikidata_titles_array, scorer=fuzz.WRatio)[1] for f in acmi_filmography]\n",
    "                wikipedia_page_result = wikipedia_page(wikidata_id)\n",
    "                if wikipedia_page_result:\n",
    "                    if len(wikipedia_page_result) == 1:\n",
    "                        time.sleep(2)\n",
    "                        r = requests.get(wikipedia_page_result[0]).text  \n",
    "                        for g in acmi_filmography:\n",
    "                            if g in r:\n",
    "                                match = wikidata_candidate\n",
    "        \n",
    "                imdb_page_result = imdb_page(wikidata_id)\n",
    "                if imdb_page_result is not None:\n",
    "                    if len(imdb_page_result) == 1:\n",
    "                        imdb_data_candidate = imdb_data.loc[imdb_data.nconst.isin([imdb_page_result[0]])]\n",
    "                        for g in acmi_filmography:\n",
    "                            if g in imdb_data_candidate.title.unique():\n",
    "                                match = wikidata_candidate\n",
    "\n",
    "        with open(hash_path, 'w') as export:\n",
    "            export.write(f\"{creator['acmi_id']} {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
